{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christamadani/Assignment3/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Nbf3pp1aKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e53b37-cbc8-40ed-e3e0-42b19691f423"
      },
      "source": [
        "#####################################################################################################################\n",
        "#   Assignment 3: K-means unsupervised learning algorithm\n",
        "#   Christopher Shatley and Christa Madani\n",
        "#####################################################################################################################\n",
        "import copy\n",
        "import pandas as pd\n",
        "import random\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/christamadani/Assignment3/main/bbchealth.txt', header=None,\n",
        "                 delimiter=\"|\")\n",
        "df.columns = ['a', 'b', 'Line']\n",
        "df.drop('a', inplace=True, axis=1)\n",
        "df.drop('b', inplace=True, axis=1)\n",
        "df = df.replace('#', '', regex=True)\n",
        "df = df.replace('@\\w+', '', regex=True)\n",
        "df = df.replace('((http)(s?)(:\\/\\/)(.*)(\\.)([\\S]*))', '', regex=True)\n",
        "df['Line'] = df['Line'].str.lower()\n",
        "df.astype('string')\n",
        "\n",
        "\n",
        "def k_means_cluster(f_k, f_df, f_r):\n",
        "\n",
        "    # Create the init centroids\n",
        "    full_set = set()\n",
        "    for name, val in f_df.itertuples():\n",
        "        # Split the text\n",
        "        text_set = set(val.split())\n",
        "        full_set = full_set.union(text_set)\n",
        "\n",
        "    # List of centroids\n",
        "    centroids = []\n",
        "    centroids_new = []\n",
        "    centroids_tweet_count = []\n",
        "\n",
        "    # We can also just split it by k, we are getting randoms of each point\n",
        "    for x in range(f_k + 1):\n",
        "\n",
        "        full_set_list = list(full_set)\n",
        "        subset = set()\n",
        "        # Size of random drop\n",
        "        centroid_size = int((full_set_list.__len__()) / (f_k + 1))\n",
        "\n",
        "        # Randomly shuffle the list?\n",
        "        if f_r:\n",
        "            random.shuffle(full_set_list)\n",
        "\n",
        "        # Take set of data from start point to end point.\n",
        "        for word in full_set_list[x * centroid_size: (full_set_list.__len__() - (f_k - x) * centroid_size)]:\n",
        "            subset = subset.union([word])\n",
        "\n",
        "        # print(subset)\n",
        "        centroids.append(copy.deepcopy(subset))\n",
        "        subset.clear()\n",
        "\n",
        "    # print(centroids)\n",
        "    # print()\n",
        "\n",
        "    # de-facto adjacency matrix.\n",
        "    centroid_pos_cur = [[0 for n1 in range(f_df.size)] for n2 in range(f_k + 1)]\n",
        "    centroid_item_cur = [0 for n1 in range(f_df.size)]\n",
        "    centroid_item_old = [1 for n1 in range(f_df.size)]\n",
        "\n",
        "    # Till converge 10 centroids, and\n",
        "    centroid_idx = 0\n",
        "    item_idx = int(0)\n",
        "\n",
        "    while centroid_item_cur != centroid_item_old:\n",
        "\n",
        "        # Reset in case\n",
        "        centroids_tweet_count.clear()\n",
        "        centroids_tweet_count = [0] * (f_k + 1)\n",
        "\n",
        "\n",
        "        # Update previous old\n",
        "        centroid_item_old = copy.deepcopy(centroid_item_cur)\n",
        "\n",
        "        # For each item\n",
        "        for name, val in f_df.itertuples():\n",
        "\n",
        "            # Check each distance in the allocated centroid if | Calculate distance to the centroid\n",
        "            current_min = 100000\n",
        "            for c_m in centroids:\n",
        "\n",
        "                # Split the text\n",
        "                text_set = set(val.split())\n",
        "                # Get the distance\n",
        "                centroid_pos_cur[centroid_idx][item_idx] = distance(text_set, c_m)\n",
        "\n",
        "                if centroid_pos_cur[centroid_idx][item_idx] < current_min:\n",
        "                    current_min = centroid_pos_cur[centroid_idx][item_idx]\n",
        "                    centroid_item_cur[item_idx] = centroid_idx\n",
        "                    # print(centroid_idx)\n",
        "                    # print(current_min)\n",
        "\n",
        "                centroid_idx = centroid_idx + 1\n",
        "\n",
        "            centroid_idx = 0\n",
        "            item_idx = item_idx + 1\n",
        "\n",
        "        # Update the centroid centers.\n",
        "        centroid_update = set()\n",
        "        item_idx = 0\n",
        "\n",
        "        # For each centroid\n",
        "        for cent in range(f_k + 1):\n",
        "\n",
        "            # For each item\n",
        "            for name, val in f_df.itertuples():\n",
        "                # Split the text to create new center\n",
        "                # print('closest centroid ' + str(centroid_item_cur[item_idx]) + 'current centroid ' + str(cent))\n",
        "                if centroid_item_cur[item_idx] == cent:\n",
        "                    text_set = set(val.split())\n",
        "                    # Here actual distance.\n",
        "                    centroid_update = centroid_update.union(text_set)\n",
        "                    # Increment the tweet count\n",
        "                    #print(centroid_idx)\n",
        "                    tval = centroids_tweet_count[centroid_idx] + 1\n",
        "                    centroids_tweet_count[centroid_idx] = tval\n",
        "                item_idx = item_idx + 1\n",
        "\n",
        "            centroid_idx = centroid_idx + 1\n",
        "            item_idx = 0\n",
        "            centroids_new.append(copy.deepcopy(centroid_update))\n",
        "            centroid_update.clear()\n",
        "\n",
        "        centroid_idx = 0\n",
        "        centroids.clear()\n",
        "        centroids = copy.deepcopy(centroids_new)\n",
        "        centroids_new.clear()\n",
        "\n",
        "        # print(centroid_item_cur)\n",
        "        # print(centroid_item_old)\n",
        "\n",
        "    SSE = sse(f_df, centroids, centroid_item_cur)\n",
        "    return centroids, SSE, f_k + 1, centroids_tweet_count\n",
        "\n",
        "\n",
        "def sse(f_df, centroids, centroid_item_cur):\n",
        "    dsqu_list = []\n",
        "    centroid_idx = 0\n",
        "    item_idx = 0\n",
        "    item_idx2 = 0\n",
        "    f_df2 = copy.deepcopy(f_df)\n",
        "\n",
        "    # Compare all points in a centroid to its neighbors, then div by the size of the centroid\n",
        "    # We do this becase we don't have a true center of a centroid, and we need the metric of how\n",
        "    # Tightly it is clustered\n",
        "    for c_m in centroids:\n",
        "        for name, val in f_df.itertuples():\n",
        "            if centroid_idx == centroid_item_cur[item_idx]:\n",
        "                for name2, val2 in f_df2.itertuples():\n",
        "                    if centroid_idx == centroid_item_cur[item_idx2]:\n",
        "                        text_set = set(val.split())\n",
        "                        text_set2 = set(val2.split())\n",
        "                        # print(\"Item Index \" + item_idx + \" Text set: \" + text_set + \" Centroid: \" + c_m)\n",
        "                        # Get the distance\n",
        "                        # dsqu_list.append(distance(text_set, c_m) ** 2)\n",
        "                        dsqu_list.append((distance(text_set, text_set2) ** 2) / len(c_m))\n",
        "                    item_idx2 = item_idx2 + 1\n",
        "                item_idx2 = 0\n",
        "            item_idx = item_idx + 1\n",
        "        item_idx = 0\n",
        "        centroid_idx = centroid_idx + 1\n",
        "\n",
        "    # print(len(dsqu_list))\n",
        "    # print(SSE)\n",
        "\n",
        "    SSE = sum(dsqu_list)\n",
        "    return SSE\n",
        "\n",
        "\n",
        "# Abstract distance calculation (I want to make this be variant.) Add param to change distance measure to EUCLIDEAN WOO.\n",
        "def distance(set1, set2):\n",
        "    l_u, l_i = jaccard_load(set1, set2)\n",
        "    l_d = jaccard(l_u, l_i)\n",
        "    return l_d\n",
        "\n",
        "\n",
        "# Jaccard Load sets\n",
        "def jaccard_load(set1, set2):\n",
        "    l_u = set1.union(set2)\n",
        "    l_i = set1.intersection(set2)\n",
        "    return l_u, l_i\n",
        "\n",
        "\n",
        "# Jaccard calculation\n",
        "def jaccard(u, i):\n",
        "    IntersectionLen = len(i)\n",
        "    UnionLen = len(u)\n",
        "    return round(1 - (int(IntersectionLen) / UnionLen), 4)\n",
        "\n",
        "\n",
        "# Run the K-means sim\n",
        "if __name__ == \"__main__\":\n",
        "    head = [\"Value of k\", \"SSE\", \"Size of each cluster\"]\n",
        "    data = []\n",
        "    cent_data = []\n",
        "    for i in range(0, 15, 2):\n",
        "        # Clusters starting from 1\n",
        "        f_centroid, f_sse, f_k, f_centroid_tweet = k_means_cluster(i, df, True)\n",
        "        c_count = 1\n",
        "        cent_str = ''\n",
        "        cent_data.clear()\n",
        "        for l_c in f_centroid_tweet:\n",
        "            cent_data.append(str(c_count) + \" : \" + str(l_c) + \" tweets\")\n",
        "            c_count = c_count + 1\n",
        "        for line in cent_data:\n",
        "            cent_str = cent_str + str(line) + \"\\n\"\n",
        "        data.append((str(f_k), str(f_sse), cent_str))\n",
        "        # print(tabulate(data, headers=head, tablefmt=\"grid\"))\n",
        "    print(tabulate(data, headers=head, tablefmt=\"grid\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------+------------------------+\n",
            "|   Value of k |     SSE | Size of each cluster   |\n",
            "+==============+=========+========================+\n",
            "|            1 | 2463.74 | 1 : 3929 tweets        |\n",
            "+--------------+---------+------------------------+\n",
            "|            3 | 1876.31 | 1 : 1194 tweets        |\n",
            "|              |         | 2 : 1208 tweets        |\n",
            "|              |         | 3 : 1527 tweets        |\n",
            "+--------------+---------+------------------------+\n",
            "|            5 | 1757.11 | 1 : 674 tweets         |\n",
            "|              |         | 2 : 785 tweets         |\n",
            "|              |         | 3 : 753 tweets         |\n",
            "|              |         | 4 : 892 tweets         |\n",
            "|              |         | 5 : 825 tweets         |\n",
            "+--------------+---------+------------------------+\n",
            "|            7 | 1680.11 | 1 : 497 tweets         |\n",
            "|              |         | 2 : 612 tweets         |\n",
            "|              |         | 3 : 587 tweets         |\n",
            "|              |         | 4 : 648 tweets         |\n",
            "|              |         | 5 : 434 tweets         |\n",
            "|              |         | 6 : 594 tweets         |\n",
            "|              |         | 7 : 557 tweets         |\n",
            "+--------------+---------+------------------------+\n",
            "|            9 | 1565.22 | 1 : 438 tweets         |\n",
            "|              |         | 2 : 326 tweets         |\n",
            "|              |         | 3 : 434 tweets         |\n",
            "|              |         | 4 : 458 tweets         |\n",
            "|              |         | 5 : 497 tweets         |\n",
            "|              |         | 6 : 512 tweets         |\n",
            "|              |         | 7 : 469 tweets         |\n",
            "|              |         | 8 : 337 tweets         |\n",
            "|              |         | 9 : 458 tweets         |\n",
            "+--------------+---------+------------------------+\n",
            "|           11 | 1480.04 | 1 : 362 tweets         |\n",
            "|              |         | 2 : 367 tweets         |\n",
            "|              |         | 3 : 337 tweets         |\n",
            "|              |         | 4 : 360 tweets         |\n",
            "|              |         | 5 : 364 tweets         |\n",
            "|              |         | 6 : 367 tweets         |\n",
            "|              |         | 7 : 404 tweets         |\n",
            "|              |         | 8 : 323 tweets         |\n",
            "|              |         | 9 : 344 tweets         |\n",
            "|              |         | 10 : 368 tweets        |\n",
            "|              |         | 11 : 333 tweets        |\n",
            "+--------------+---------+------------------------+\n",
            "|           13 | 1479.31 | 1 : 316 tweets         |\n",
            "|              |         | 2 : 296 tweets         |\n",
            "|              |         | 3 : 306 tweets         |\n",
            "|              |         | 4 : 312 tweets         |\n",
            "|              |         | 5 : 334 tweets         |\n",
            "|              |         | 6 : 333 tweets         |\n",
            "|              |         | 7 : 253 tweets         |\n",
            "|              |         | 8 : 311 tweets         |\n",
            "|              |         | 9 : 307 tweets         |\n",
            "|              |         | 10 : 302 tweets        |\n",
            "|              |         | 11 : 301 tweets        |\n",
            "|              |         | 12 : 283 tweets        |\n",
            "|              |         | 13 : 275 tweets        |\n",
            "+--------------+---------+------------------------+\n",
            "|           15 | 1441.47 | 1 : 247 tweets         |\n",
            "|              |         | 2 : 301 tweets         |\n",
            "|              |         | 3 : 214 tweets         |\n",
            "|              |         | 4 : 264 tweets         |\n",
            "|              |         | 5 : 248 tweets         |\n",
            "|              |         | 6 : 255 tweets         |\n",
            "|              |         | 7 : 255 tweets         |\n",
            "|              |         | 8 : 261 tweets         |\n",
            "|              |         | 9 : 234 tweets         |\n",
            "|              |         | 10 : 320 tweets        |\n",
            "|              |         | 11 : 238 tweets        |\n",
            "|              |         | 12 : 290 tweets        |\n",
            "|              |         | 13 : 283 tweets        |\n",
            "|              |         | 14 : 265 tweets        |\n",
            "|              |         | 15 : 254 tweets        |\n",
            "+--------------+---------+------------------------+\n"
          ]
        }
      ]
    }
  ]
}